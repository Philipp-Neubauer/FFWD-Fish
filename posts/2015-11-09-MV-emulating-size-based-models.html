<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>FFWD fish blog</title>
        <link rel="stylesheet" type="text/css" href="../css/style.css" />
    </head>
    <body>
        <header class="navigation">
            <div class="navigation-wrapper">
                <a href="javascript:void(0)" class="logo">
                    <img src="../images/logo_Sam.png" alt="Logo Image" align="middle">
                </a>
                <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">MENU</a>
                <div class="nav">
                    <ul id="js-navigation-menu" class="navigation-menu show">
                        <li class="nav-link"><a href="../index.html">Home</a></li>
                        <li class="nav-link"><a href="../archive.html">Archive</a></li>
                        <li class="nav-link"><a href="../about.html">About</a></li>
                        <li class="nav-link"><a href="../contact.html">Contact</a></li>
                        <li class="nav-link"><a href="https://github.com/Philipp-Neubauer/FFWD-Fish"> <img src="../images/Octocat_small.png" width="66.66" height="55.41" align="top"></a></li>
                    </ul>
                </div>
                 
            </div>
        </header>
        <main>
            
            <div class="container">
            <div class="info">
    <br>
    
    Posted on November  9, 2015
    
        by Philipp Neubauer
    
</div>

<p>In a <a href="2015-05-05-emulation_using_GPs.html">previous try</a>, I played with history matching, a method for fitting complex computer simulation models to data <span class="citation">(Kennedy and O’Hagan 2001)</span>. My first try (hack?) was a rather naive attempt to understand how history matching works in general, and to get an idea of its usefulness for fitting size-spectra to data. But I didn't really make a serious attempt at going through a history matching excersize that might produce useful insights into possible parameter values of uncertain size-spectrum parameters.</p>
<p>So for this try I will make a more serious attempt at fitting size-spectra, using a simulated trait based model with multivariate outputs (species bioamss levels, possibly others). I will start with a very course evaluation of active parameters: those are the parameters that are considered uncertain and which fundametally alter the dynamics of the system (those whose variation does not alter the system behaviour are obviously somewhat redundant).</p>
<p>One thing that did become clear in the first round of history matching was that univariate emulators are very inefficient. In this post, I will try more efficient multi-variate emulators that are based on assumption of separability of input and output variances <span class="citation">(Rougier 2008)</span>. I will also more explicitly query if the emulator makes reasonable predictions away from data, which is be cruicial if this type of method is to work for ecosystem models.</p>
<h1 id="setting-up-a-trait-based-model">Setting up a trait based model</h1>
<p>I will use mizer to set up a trait based model, and estiamte its (active) parameters. The mizer vignette provides a convenient reminder how to set this up:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">no_sp =<span class="st"> </span><span class="dv">10</span>
min_w_inf &lt;-<span class="st"> </span><span class="dv">10</span>
max_w_inf &lt;-<span class="st"> </span><span class="fl">1e5</span>
w_inf &lt;-<span class="st"> </span><span class="dv">10</span>^<span class="kw">seq</span>(<span class="dt">from=</span><span class="kw">log10</span>(min_w_inf), <span class="dt">to =</span> <span class="kw">log10</span>(max_w_inf), <span class="dt">length=</span><span class="dv">10</span>)
knife_edges &lt;-<span class="st"> </span>w_inf *<span class="st"> </span><span class="fl">0.05</span>

truth &lt;-<span class="st"> </span><span class="kw">set_trait_model</span>(<span class="dt">no_sp =</span> no_sp, 
                         <span class="dt">r_pp =</span> <span class="dv">10</span>,
                         <span class="dt">kappa =</span> <span class="fl">0.01</span>,
                         <span class="dt">h=</span><span class="dv">40</span>,
                         <span class="dt">beta =</span> <span class="dv">160</span>,
                         <span class="dt">sigma=</span> <span class="dv">2</span>,
                         <span class="dt">f_0 =</span> <span class="fl">0.8</span>,
                         <span class="dt">min_w_inf =</span> min_w_inf, 
                         <span class="dt">max_w_inf =</span> max_w_inf,
                         <span class="dt">knife_edge_size =</span> knife_edges)
  
sim &lt;-<span class="st"> </span><span class="kw">project</span>(truth, <span class="dt">t_max =</span> <span class="dv">100</span>, <span class="dt">dt=</span><span class="fl">0.2</span>, <span class="dt">effort =</span> <span class="fl">0.4</span>)

<span class="kw">plot</span>(sim)</code></pre></div>
<div class="figure">
<img src="2015-11-09-MV-emulating-size-based-models_files/figure-markdown/simulation-1.png" alt />

</div>
<p>Looks like this arbitrary trait based model has converged to a stable solution. I now use mean biomass (though I could use just the last year) and total catch over the last 25 years as data for the calibration.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">biom &lt;-<span class="st"> </span><span class="kw">colMeans</span>(<span class="kw">getBiomass</span>(sim)[<span class="dv">75</span>:<span class="dv">100</span>,])
catch &lt;-<span class="st"> </span><span class="kw">colSums</span>(<span class="kw">getYield</span>(sim)[<span class="dv">75</span>:<span class="dv">100</span>,])</code></pre></div>
<p>Since I want to do a sweep for active parameters first, I take all parameters that may be considered uncertain in the input dataset. Note taht this leads to a large hypercube of unknowns, even with a small grid of possible values across all unknowns. I.e., with 8 unknowns, the grid with <span class="math inline"><em>n</em> = 3</span> evaluations for each variable is <span class="math inline">3<sup>8</sup> × 8</span>. Although it can be efficiently parallelised, the runs required to get this many outputs take a considerable amount of time. just adding one more point per parameter would mean pushing the enveloppe (about 1000CPU hours), any more would be prohibitive. How then to make this tractable? The answer is to run a small grid (i.e., <span class="math inline"><em>n</em> = 3</span>) and then use a fast emulator to interpolate model outputs and discard those that we believe are implausible. Here's a start:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cube_pred &lt;-<span class="st"> </span><span class="dv">4</span>

preds &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="co">#seq(2/3,0.9,l=cube_pred),</span>
            <span class="co">#seq(2/3,0.9,l=cube_pred),     </span>
            <span class="co">#seq(0.2,1,l=cube_pred),</span>
            <span class="dt">h =</span> <span class="kw">seq</span>(<span class="dv">3</span>,<span class="dv">50</span>,<span class="dt">l=</span>cube_pred),
            <span class="dt">beta =</span> <span class="kw">seq</span>(<span class="dv">50</span>,<span class="dv">1000</span>,<span class="dt">l=</span>cube_pred),
            <span class="dt">r_pp =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">100</span>,<span class="dt">l=</span>cube_pred),
            <span class="dt">kappa =</span> <span class="kw">seq</span>(<span class="fl">1e-4</span>,<span class="fl">1e-2</span>,<span class="dt">l=</span>cube_pred),
            <span class="dt">sigma =</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dt">l=</span>cube_pred))
    
pred_pre_list &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(preds)
pred_list &lt;-<span class="st"> </span><span class="kw">split</span>(pred_pre_list,<span class="dv">1</span>:<span class="kw">nrow</span>(pred_pre_list))       

<span class="kw">system.time</span>( simdat &lt;-<span class="st"> </span>parallel::<span class="kw">mclapply</span>(pred_list,
                             function(inputs){
                               this.setup &lt;-<span class="st"> </span><span class="kw">set_trait_model</span>(<span class="dt">no_sp =</span> no_sp, 
                                                             <span class="dt">r_pp =</span> inputs$r_pp,
                                                             <span class="dt">kappa =</span> inputs$kappa,
                                                             <span class="dt">h=</span>inputs$h,
                                                             <span class="dt">beta =</span> inputs$beta,
                                                             <span class="dt">sigma=</span> inputs$sigma,
                                                             <span class="dt">q=</span><span class="fl">0.9</span>,<span class="co">#inputs$q,</span>
                                                             <span class="dt">p=</span><span class="fl">0.75</span>,<span class="co">#inputs$p,</span>
                                                             <span class="dt">n=</span><span class="dv">2</span>/<span class="dv">3</span>,<span class="co">#inputs$n,</span>
                                                             <span class="dt">f_0 =</span> <span class="fl">0.8</span>,
                                                             <span class="dt">min_w_inf =</span> min_w_inf, 
                                                             <span class="dt">max_w_inf =</span> max_w_inf,
                                                             <span class="dt">knife_edge_size =</span> knife_edges)
                               
                               this.sim &lt;-<span class="st"> </span><span class="kw">project</span>(this.setup, <span class="dt">t_max =</span> <span class="dv">100</span>, <span class="dt">dt=</span><span class="fl">0.2</span>, <span class="dt">effort =</span> <span class="fl">0.4</span>)
                               this.biom &lt;-<span class="st"> </span><span class="kw">colMeans</span>(<span class="kw">getBiomass</span>(this.sim)[<span class="dv">75</span>:<span class="dv">100</span>,])
                               this.biom
                             },<span class="dt">mc.cores =</span> <span class="dv">4</span>))</code></pre></div>
<pre><code>##      user    system   elapsed 
## 17940.710    37.464  6297.544</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim_data &lt;-<span class="st"> </span><span class="kw">do.call</span>(<span class="st">'rbind'</span>,simdat)</code></pre></div>
<p>I will keep some of the data here to evaluate the emulator - so I'll make a training and a test set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="kw">nrow</span>(sim_data), <span class="dt">size =</span> <span class="fl">0.9</span>*<span class="kw">nrow</span>(sim_data))
test &lt;-<span class="st"> </span><span class="kw">which</span>(!(<span class="dv">1</span>:<span class="kw">nrow</span>(sim_data)) %in%<span class="st"> </span>train)

sim_data_train &lt;-<span class="st"> </span>sim_data[train,]
sim_data_test &lt;-<span class="st"> </span>sim_data[test,]</code></pre></div>
<p>Time to think about how to emulate this model. In geenral, species are linked and catch is linked to biomass (trivially in this case), so uni-variate emulation seems like the wrong appraoch. An alternative is multivariate emulation. In aprticular, the method described in <span class="citation">Rougier (2008)</span> may be appropriate here, since variability for the outputs is probably similar in output space (although I am not sure how similar <span class="math inline"><em>Σ</em><sub><em>B</em></sub></span> and <span class="math inline"><em>Σ</em><sub><em>C</em></sub></span> will be with repsect to variability in the inputs.)</p>
<p>First, source Johnathan Rougiers code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;include/GP_emulators/OPE.R&quot;</span>)</code></pre></div>
<p>Next I set up a naive set of regressors that define a Gaussian process mean, in this case I use a 3rd order polynomial:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">input_reg &lt;-<span class="st"> </span>function(inputs) {
  out &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,inputs,inputs^<span class="dv">2</span>,inputs^<span class="dv">3</span>)
  <span class="kw">rownames</span>(out) &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">'input %d'</span>,<span class="dv">1</span>:<span class="kw">nrow</span>(<span class="kw">as.matrix</span>(inputs)))
  <span class="kw">as.matrix</span>(out)
}</code></pre></div>
<p>Next, set up the covariance function in the inputs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(emulator)

input_var &lt;-<span class="st"> </span>function(inputs,<span class="dt">finputs=</span><span class="ot">NULL</span>) {
  cmat &lt;-<span class="st"> </span><span class="kw">corr.matrix</span>(inputs,<span class="dt">yold=</span>finputs,<span class="dt">scales=</span><span class="dv">10</span>)
  if(!<span class="kw">is.null</span>(finputs)) cmat &lt;-<span class="st"> </span><span class="kw">t</span>(cmat)
  cmat
}</code></pre></div>
<p>For the method to work, we need to specify regressors on the outputs, along with a covariance, computed from the points where the simulator is evaluated (i.e., where we calculate the biomass - at w_inf for each species). I use the same as for the inputs here as I have little prior idea, and the polynomial seems a flexible start. Also, for the output covariance, I will start with an exponential covariance for simplicity. Later, I will try to learn the scale parameters of both matrices in order to optimise the emulator.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out_grid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="kw">log10</span>(min_w_inf), <span class="dt">to =</span> <span class="kw">log10</span>(max_w_inf), <span class="dt">length=</span><span class="dv">10</span>)

output_reg &lt;-<span class="st"> </span>function(outputs) {
  out &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,outputs,outputs^<span class="dv">2</span>,outputs^<span class="dv">3</span>)
  <span class="kw">rownames</span>(out) &lt;-<span class="st"> </span><span class="kw">sprintf</span>(<span class="st">'outputs %d'</span>,<span class="dv">1</span>:<span class="kw">nrow</span>(<span class="kw">as.matrix</span>(outputs)))
  <span class="kw">as.matrix</span>(out)
}

output_regs &lt;-<span class="st"> </span><span class="kw">output_reg</span>(out_grid)

output_var &lt;-<span class="st"> </span><span class="kw">corr.matrix</span>(<span class="kw">as.matrix</span>(out_grid),<span class="dt">scales=</span><span class="dv">1</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> vr &lt;-<span class="st"> </span><span class="kw">ncol</span>(<span class="kw">input_reg</span>(pred_pre_list))
 vs &lt;-<span class="st"> </span><span class="kw">ncol</span>(output_regs)
 ms &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, vr *<span class="st"> </span>vs)
 V &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>^<span class="dv">2</span>, vr *<span class="st"> </span>vs)
   a &lt;-<span class="st"> </span><span class="dv">1</span>
   d &lt;-<span class="st"> </span><span class="dv">1</span>
   NIG &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">m =</span> ms, <span class="dt">V =</span> V, <span class="dt">a =</span> a, <span class="dt">d =</span> d)
 
 myOPE &lt;-<span class="st"> </span><span class="kw">initOPE</span>(<span class="dt">gr =</span> input_reg, 
                  <span class="dt">kappar =</span> input_var, 
                  <span class="dt">Gs =</span> output_regs , 
                  <span class="dt">Ws =</span> output_var, 
                  <span class="dt">NIG =</span> NIG)

 <span class="co"># stdise is a normal centering and fixing to [-1,1], whereas stdise_wrt(x,y) is with respect to the mean and max of y </span>
 
m &lt;-<span class="st"> </span>as.matrix
stdise &lt;-<span class="st"> </span>function(x) <span class="kw">apply</span>(x,<span class="dv">2</span>,function(y) (y-<span class="kw">mean</span>(y))/<span class="kw">max</span>(y))
stdise_wrt &lt;-<span class="st"> </span>function(x,z) <span class="kw">sapply</span>(<span class="dv">1</span>:<span class="kw">ncol</span>(x), function(y) (x[,y]-<span class="kw">mean</span>(z[,y]))/<span class="kw">max</span>(z[,y]))
  
ins &lt;-<span class="st"> </span><span class="kw">stdise</span>(<span class="kw">as.matrix</span>(pred_pre_list[train,]))
ins_test &lt;-<span class="st"> </span><span class="kw">stdise_wrt</span>(<span class="kw">m</span>(pred_pre_list[test,]),<span class="kw">m</span>(pred_pre_list[train,]))

outs &lt;-<span class="st"> </span>(sim_data_train-<span class="kw">mean</span>(sim_data_train))/<span class="kw">max</span>(sim_data_train)
tests &lt;-<span class="st"> </span>(sim_data_test-<span class="kw">mean</span>(sim_data_train))/<span class="kw">max</span>(sim_data_train)

myOPE &lt;-<span class="st"> </span><span class="kw">adjustOPE</span>(myOPE, <span class="dt">R =</span> ins, <span class="dt">Y =</span> outs)

<span class="kw">system.time</span>(pp1 &lt;-<span class="st"> </span><span class="kw">predictOPE</span>(myOPE, <span class="dt">Rp =</span> ins_test, <span class="dt">type=</span><span class="st">'EV'</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.731   0.088   0.820</code></pre>
<p>For these inputs, it seems as though the emulator does well in interpolating the results from the size-spectrum for most of the test set, but some are awefully off:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">quantile</span>(<span class="kw">abs</span>((pp1$mu-tests)/tests)*<span class="dv">100</span>)</code></pre></div>
<pre><code>##           0%          25%          50%          75%         100% 
## 9.471410e-04 6.773127e-01 1.934616e+00 5.404830e+00 1.191413e+03</code></pre>
<p>What's this related to?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># worst prediction</span>
worst &lt;-<span class="st"> </span><span class="kw">which.max</span>(<span class="kw">rowMeans</span>(<span class="kw">abs</span>((pp1$mu-tests)/tests)*<span class="dv">100</span>))
<span class="co"># severe under-prediction in the intermediate w_inf trait biomass</span>
((pp1$mu[worst,]-tests[worst,])/tests[worst,])*<span class="dv">100</span></code></pre></div>
<pre><code>##            1            2            3            4            5 
##    0.3597241  -15.8464317   -2.7385533 1191.4130197  -10.9402668 
##            6            7            8            9           10 
##    3.6255405   -9.6846286   -1.8168665    0.9253876    0.9589792</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># not really related to the actual test case biomass relative to the training set mean biomass</span>

ins[worst,]</code></pre></div>
<pre><code>##          h       beta       r_pp      kappa      sigma 
## -0.1578574  0.4700145  0.4919544  0.4951792  0.1088189</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(ins,<span class="dv">2</span>,quantile)</code></pre></div>
<pre><code>##               h       beta       r_pp      kappa      sigma
## 0%   -0.4711907 -0.4799855 -0.4980456 -0.4948208 -0.3356255
## 25%  -0.4711907 -0.1633189 -0.4980456 -0.4948208 -0.3356255
## 50%   0.1554759  0.1533478  0.1619544 -0.1648208  0.1088189
## 75%   0.4688093  0.4700145  0.4919544  0.4951792  0.3310411
## 100%  0.4688093  0.4700145  0.4919544  0.4951792  0.3310411</code></pre>
<p>Admittedly, I do not quite get where this is coming from - r_pp and sigma are high, h is low in the inputs that lead to the poor prediction, but kappa is low, yet the biomass at the inter-mediate size-level is under-estimated in the emulator.</p>
<h2 id="references">References</h2>
<p><br></p>
<div id="references" class="references">
<div id="ref-kennedy:2001:bayesian">
<p>Kennedy, M. C., and A. O’Hagan. 2001. Bayesian calibration of computer models. Journal of the Royal Statistical Society. Series B, Statistical Methodology:425–464.</p>
</div>
<div id="ref-rougier:2008:efficient">
<p>Rougier, J. 2008. Efficient emulators for multivariate deterministic functions. Journal of Computational and Graphical Statistics 17:827–843.</p>
</div>
</div>

            </div>
        </main>
        <footer>
        </footer>
        <script src="//code.jquery.com/jquery-2.1.3.min.js"></script>
        <script>
        $(document).ready(function() {
        $('#js-navigation-menu').removeClass("show");
        $('#js-mobile-menu').on('click', function(e) {
        e.preventDefault();
        $('#js-navigation-menu').slideToggle(function(){
        if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
        }
        });
        });
        });
        </script>
    </body>
</html>